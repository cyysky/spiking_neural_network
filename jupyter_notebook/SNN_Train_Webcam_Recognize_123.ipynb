{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974f303e-87bc-4eae-a8ef-ce8e76b3e980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from 'model_state_dict.pth'\n",
      "No existing data found.\n",
      "Press 's' to save an image and start training, 'r' to predict, 'q' to quit.\n",
      "Model saved as 'model_state_dict.pth'\n"
     ]
    }
   ],
   "source": [
    "#SPIKING NEURAL NETWORK WEBCAM DIGIT RECOGNITION\n",
    "#Author : Chong Yoe Yat\n",
    "#Date : 2024SEP16\n",
    "#Spiking Neural Network Train with webcam to recognize 1,2,3\n",
    "#https://www.youtube.com/watch?v=X0qicnMrYSE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import norse.torch as norse\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#Spiking Neural Network Model\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)    # First layer: Fully connected\n",
    "        self.lif1 = norse.LIFCell()                 # First spiking layer\n",
    "        self.fc2 = nn.Linear(128, 64)         # Second layer: Fully connected\n",
    "        self.lif2 = norse.LIFCell()                 # Second spiking layer\n",
    "        self.fc3 = nn.Linear(64, 10)          # Output layer for 10 classes (MNIST)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the image (28x28) to a vector of size 784\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        \n",
    "        # First layer: Linear + Spiking activation\n",
    "        x = self.fc1(x)\n",
    "        z1, _ = self.lif1(x)  # LIF Layer: Spiking computation\n",
    "        \n",
    "        # Second layer: Linear + Spiking activation\n",
    "        x = self.fc2(z1)\n",
    "        z2, _ = self.lif2(x)\n",
    "        \n",
    "        # Output layer (no spiking in the final layer)\n",
    "        x = self.fc3(z2)\n",
    "        return x\n",
    "\n",
    "# Helper functions\n",
    "def save_image(image, label, folder='data'):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    filename = os.path.join(folder, f'{label}_{int(time.time())}.png')\n",
    "    cv2.imwrite(filename, image)\n",
    "    print(f'Saved {filename}')\n",
    "\n",
    "def load_images_from_folder(folder='data'):\n",
    "    data, labels = [], []\n",
    "    \n",
    "    if os.path.exists(folder):\n",
    "        for filename in os.listdir(folder):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            # Assuming filenames are saved as \"<label>_<timestamp>.png\"\n",
    "            label = int(filename.split('_')[0])\n",
    "            \n",
    "            image = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE) \n",
    "            data.append(image)\n",
    "            labels.append(label)\n",
    "        print(f\"Loaded {len(data)} images from {folder}\")\n",
    "    else:\n",
    "        print(\"No existing data found.\")\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "        \n",
    "def load_model():\n",
    "    model = SNN()\n",
    "    if os.path.exists('model_state_dict.pth'):\n",
    "        model.load_state_dict(torch.load('model_state_dict.pth'))\n",
    "        print(\"Model loaded from 'model_state_dict.pth'\")\n",
    "    else:\n",
    "        print(\"No pre-trained model found. Starting fresh.\")\n",
    "    return model\n",
    "\n",
    "# non batch training\n",
    "def train_model(model, data, labels, epochs=100):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (img, label) in enumerate(zip(data, labels)):\n",
    "            img = torch.Tensor(img).unsqueeze(0)  # Add batch dimension\n",
    "            label = torch.tensor([label])\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(img)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# training with batch\n",
    "def train_model(model, data, labels, batch_size=5, epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = len(data)\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            \n",
    "            # Convert to tensors and create batches\n",
    "            batch_data = torch.Tensor(batch_data)\n",
    "            batch_labels = torch.Tensor(batch_labels).long()  # Convert labels to LongTensor\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output, batch_labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Batch {i//batch_size+1}/{(dataset_size+batch_size-1)//batch_size}, Loss: {loss.item()}')\n",
    "        scheduler.step()\n",
    "\n",
    "def predict(model, image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = torch.Tensor(image).unsqueeze(0)  # Add batch dimension\n",
    "        output = model(img)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        return predicted.item()\n",
    "\n",
    "def main():\n",
    "    model = load_model()\n",
    "    data, labels = load_images_from_folder()  # Load previous data\n",
    "    #train_model(model, data, labels)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(\"Press 's' to save an image and start training, 'r' to predict, 'q' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "                \n",
    "        if key in range(48, 58):  # Keys '0' to '9'\n",
    "            label = key - 48  # Convert ASCII to digit\n",
    "            image_resized = cv2.resize(gray, (28, 28))  # Resize to 28x28\n",
    "            save_image(image_resized, label)\n",
    "            data.append(image_resized)  # Append new data\n",
    "            labels.append(label)\n",
    "            \n",
    "        elif key == ord('t'):  # Predict\n",
    "            train_model(model, data, labels,epochs=10)\n",
    "            \n",
    "        elif key == ord('r'):  # Predict\n",
    "            image_resized = cv2.resize(gray, (28, 28))\n",
    "            pred = predict(model, image_resized)\n",
    "            print(f'Predicted digit: {pred}')\n",
    "        \n",
    "        elif key == ord('q'):  # Quit and save model\n",
    "            torch.save(model.state_dict(), 'model_state_dict.pth')\n",
    "            print(\"Model saved as 'model_state_dict.pth'\")\n",
    "            break\n",
    "            \n",
    "        image_resized = cv2.resize(gray, (28, 28))\n",
    "        pred = predict(model, image_resized)\n",
    "        # font\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        # org\n",
    "        org = (50, 50)\n",
    "        # fontScale\n",
    "        fontScale = 1\n",
    "        # Blue color in BGR\n",
    "        color = (255, 0, 0)\n",
    "        # Line thickness of 2 px\n",
    "        thickness = 2\n",
    "        # Using cv2.putText() method\n",
    "        gray = cv2.putText(gray, str(pred), org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.imshow('Webcam', gray)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3d356-54ca-49c9-aafe-f416fe2e506c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
